{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 42177 — PCA → Logistic Regression (clean + degraded), manifest-driven\n",
        "- Uses manifest and preprocessed data.\n",
        "- Streams for scaler/PCA to avoid RAM spikes.\n",
        "- Evaluates on clean test and all degraded test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## 0) Imports and global config\n",
        "\n",
        "# %%\n",
        "import os, json, time, random, numpy as np, pandas as pd\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------- CONFIG -------------------------\n",
        "# Paths\n",
        "MANIFEST = r\"D:\\LocalUser\\42177 Project\\data\\manifest.csv\"\n",
        "ROOT_CLEAN = r\"D:\\LocalUser\\42177 Project\\data\\clean\"\n",
        "ROOT_DEG = r\"D:\\LocalUser\\42177 Project\\data\\degraded\"\n",
        "ART = r\"D:\\LocalUser\\42177 Project\\artifacts\"\n",
        "\n",
        "# hyperparameters\n",
        "TARGET_HW = (256, 256)\n",
        "SEED = 42177\n",
        "BATCH = 1024          # streaming batch for scaler/IPCA/transform\n",
        "NCOMP = 256           # PCA components\n",
        "\n",
        "os.makedirs(ART, exist_ok=True)\n",
        "random.seed(SEED); np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Manifest and label set\n",
        "- Manifest defines the project’s class list and fixed splits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['babesia',\n",
              " 'leishmania',\n",
              " 'plasmodium',\n",
              " 'toxoplasma1000x',\n",
              " 'toxoplasma400x',\n",
              " 'trichomonad',\n",
              " 'trypanosome']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%\n",
        "df = pd.read_csv(MANIFEST)  # expects: id, filepath, label, subset, mag\n",
        "LABELS = sorted(df['label'].unique().tolist())  # deterministic order\n",
        "LABELS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) I/O helpers\n",
        "- Read RGB 256×256 images.\n",
        "- Stream batches for train to reduce RAM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------- DATA HELPERS -------------------------\n",
        "EXTS = ('.png','.jpg','.jpeg','.tif','.tiff','.bmp')\n",
        "\n",
        "def _list_images(folder):\n",
        "    if not os.path.isdir(folder): return []\n",
        "    return [os.path.join(folder,f) for f in os.listdir(folder) if f.lower().endswith(EXTS)]\n",
        "\n",
        "def manifest_labels(manifest_path):\n",
        "    df = pd.read_csv(manifest_path)\n",
        "    # use sorted labels for deterministic order across environments\n",
        "    return sorted(df['label'].unique().tolist())\n",
        "\n",
        "LABELS = manifest_labels(MANIFEST)\n",
        "\n",
        "def stream_clean_subset(subset, target=TARGET_HW):\n",
        "    \"\"\"Yield (Xb, yb) batches from data/clean/<subset>/<label> with Xb in [0,1], flattened.\"\"\"\n",
        "    for lab in LABELS:\n",
        "        folder = os.path.join(ROOT_CLEAN, subset, lab)\n",
        "        paths = _list_images(folder)\n",
        "        for i in range(0, len(paths), BATCH):\n",
        "            chunk = paths[i:i+BATCH]\n",
        "            Xb, yb = [], []\n",
        "            for p in chunk:\n",
        "                im = Image.open(p).convert(\"RGB\").resize(target)\n",
        "                arr = np.asarray(im, dtype=np.float32) / 255.0\n",
        "                Xb.append(arr.reshape(-1))   # flatten 256*256*3\n",
        "                yb.append(lab)\n",
        "            if Xb:\n",
        "                yield np.stack(Xb), np.array(yb)\n",
        "\n",
        "def load_clean_subset_full(subset, target=TARGET_HW):\n",
        "    \"\"\"Load entire subset into memory (used after PCA transform, which is small).\"\"\"\n",
        "    X, y = [], []\n",
        "    for lab in LABELS:\n",
        "        folder = os.path.join(ROOT_CLEAN, subset, lab)\n",
        "        for p in _list_images(folder):\n",
        "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
        "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
        "            X.append(arr.reshape(-1))\n",
        "            y.append(lab)\n",
        "    return np.stack(X), np.array(y)\n",
        "\n",
        "def load_degraded_full(cond, subset=\"test\", target=TARGET_HW):\n",
        "    \"\"\"Load degraded images for one condition into memory (then PCA-transform).\"\"\"\n",
        "    X, y = [], []\n",
        "    base = os.path.join(ROOT_DEG, cond, subset)\n",
        "    if not os.path.isdir(base): return None, None\n",
        "    for lab in LABELS:\n",
        "        folder = os.path.join(base, lab)\n",
        "        for p in _list_images(folder):\n",
        "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
        "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
        "            X.append(arr.reshape(-1))\n",
        "            y.append(lab)\n",
        "    if not X: return None, None\n",
        "    return np.stack(X), np.array(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Metric packer\n",
        "- Accuracy, macro Precision/Recall/F1, and Confusion Matrix with fixed label order.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def evaluate_and_pack(y_true, y_pred, labels=LABELS):\n",
        "    y_true = np.asarray(y_true).astype(str)\n",
        "    y_pred = np.asarray(y_pred).astype(str)\n",
        "    labels = [str(x) for x in labels]\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
        "    )\n",
        "    return {\n",
        "        \"Accuracy\": float(acc),\n",
        "        \"Precision\": float(prec),\n",
        "        \"Recall\": float(rec),\n",
        "        \"F1\": float(f1),\n",
        "        \"ConfusionMatrix\": cm.tolist(),\n",
        "        \"Labels\": labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Fit StandardScaler and IncrementalPCA on train (streaming)\n",
        "- Two passes: first for scaler, second for PCA on standardized batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaler fitted on 14390 samples.\n",
            "Scaler fitted (global stream).\n",
            "IPCA fitted (n_components=128).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ------------------------- TRAIN (SCALER + IPCA + LR) -------------------------\n",
        "t_total = time.time()\n",
        "\n",
        "# 1) Fit scaler on train via streaming (no big RAM usage)\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "n_seen = 0\n",
        "for Xb, _ in stream_clean_subset(\"train\"):\n",
        "    scaler.partial_fit(Xb)\n",
        "    n_seen += len(Xb)\n",
        "print(f\"Scaler fitted on {n_seen} samples.\")\n",
        "\n",
        "# 2) Fit IncrementalPCA (global stream, safe batch size)\n",
        "BATCH = 1024\n",
        "NCOMP = 128\n",
        "\n",
        "def stream_clean_global(subset, batch=BATCH, target=TARGET_HW):\n",
        "    \"\"\"Yield flattened batches combining all classes (no small per-class batches).\"\"\"\n",
        "    paths = []\n",
        "    for lab in LABELS:\n",
        "        d = os.path.join(ROOT_CLEAN, subset, lab)\n",
        "        paths += [(os.path.join(d, f), lab)\n",
        "                  for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
        "    for i in range(0, len(paths), batch):\n",
        "        chunk = paths[i:i+batch]\n",
        "        Xb, yb = [], []\n",
        "        for p, lab in chunk:\n",
        "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
        "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
        "            Xb.append(arr.reshape(-1))\n",
        "            yb.append(lab)\n",
        "        yield np.stack(Xb), np.array(yb)\n",
        "\n",
        "# 1st pass — fit scaler incrementally\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "for Xb, _ in stream_clean_global(\"train\"):\n",
        "    scaler.partial_fit(Xb)\n",
        "print(\"Scaler fitted (global stream).\")\n",
        "\n",
        "# 2nd pass — fit IncrementalPCA incrementally\n",
        "ipca = IncrementalPCA(n_components=NCOMP, batch_size=BATCH)\n",
        "for Xb, _ in stream_clean_global(\"train\"):\n",
        "    Xb_std = scaler.transform(Xb)\n",
        "    if Xb_std.shape[0] < NCOMP:   # ensure n_samples ≥ n_components\n",
        "        continue\n",
        "    ipca.partial_fit(Xb_std)\n",
        "print(f\"IPCA fitted (n_components={NCOMP}).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression...\n",
            "Training completed in 0.12 minutes\n"
          ]
        }
      ],
      "source": [
        "# 3) Transform TRAIN/TEST in streams → keep only PCA features (N × NCOMP)\n",
        "Xtr_chunks, ytr_chunks = [], []\n",
        "for Xb, yb in stream_clean_global(\"train\"):\n",
        "    Xb = Xb.astype(np.float32, copy=False)\n",
        "    Xb_std = scaler.transform(Xb)\n",
        "    Xb_pca = ipca.transform(Xb_std)\n",
        "    Xtr_chunks.append(Xb_pca.astype(np.float32, copy=False))\n",
        "    ytr_chunks.append(yb)\n",
        "Xtr = np.vstack(Xtr_chunks)\n",
        "ytr = np.concatenate(ytr_chunks)\n",
        "\n",
        "Xte_chunks, yte_chunks = [], []\n",
        "for Xb, yb in stream_clean_global(\"test\"):\n",
        "    Xb = Xb.astype(np.float32, copy=False)\n",
        "    Xb_std = scaler.transform(Xb)\n",
        "    Xb_pca = ipca.transform(Xb_std)\n",
        "    Xte_chunks.append(Xb_pca.astype(np.float32, copy=False))\n",
        "    yte_chunks.append(yb)\n",
        "Xte = np.vstack(Xte_chunks)\n",
        "yte = np.concatenate(yte_chunks)\n",
        "\n",
        "# 4) Train multinomial Logistic Regression on PCA features\n",
        "clf = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    solver=\"lbfgs\",\n",
        "    multi_class=\"multinomial\",\n",
        "    n_jobs=-1,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "print(\"Training Logistic Regression...\")\n",
        "t_train = time.time()\n",
        "clf.fit(Xtr, ytr)\n",
        "train_secs = time.time() - t_train\n",
        "print(f\"Training completed in {train_secs/60:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Evaluate on clean test and save\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_clean.json\n"
          ]
        }
      ],
      "source": [
        "# ------------------------- EVAL: CLEAN TEST -------------------------\n",
        "t_eval = time.time()\n",
        "yhat = clf.predict(Xte)\n",
        "test_secs = time.time() - t_eval\n",
        "res_clean = evaluate_and_pack(yte, yhat, LABELS)\n",
        "res_clean[\"ElapsedTrainSeconds\"] = round(train_secs, 2)\n",
        "res_clean[\"ElapsedTestSeconds\"]  = round(test_secs, 2)\n",
        "\n",
        "with open(os.path.join(ART, \"results_logreg_clean.json\"), \"w\") as f:\n",
        "    json.dump(res_clean, f, indent=2)\n",
        "print(\"Saved:\", os.path.join(ART, \"results_logreg_clean.json\"))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Evaluate on all degraded test sets (if present)\n",
        "- Loads each condition, transforms with same scaler/PCA, evaluates, saves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_gaussian_blur_s1.0.json\n",
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_gaussian_blur_s2.0.json\n",
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_gaussian_noise_s15.json\n",
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_gaussian_noise_s5.json\n",
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_jpeg_q20.json\n",
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_jpeg_q40.json\n",
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_jpeg_q60.json\n",
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_motion_blur_k5.json\n",
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_resolution_x2.json\n",
            "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_logreg_resolution_x4.json\n",
            "Total wall time: 75.5 min\n"
          ]
        }
      ],
      "source": [
        "# ------------------------- EVAL: DEGRADED TESTS (streaming, low RAM) -------------------------\n",
        "def stream_degraded_global(cond, subset=\"test\", batch=BATCH, target=TARGET_HW):\n",
        "    paths = []\n",
        "    base = os.path.join(ROOT_DEG, cond, subset)\n",
        "    if not os.path.isdir(base):\n",
        "        return\n",
        "    for lab in LABELS:\n",
        "        d = os.path.join(base, lab)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "        paths += [(os.path.join(d, f), lab) for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
        "    for i in range(0, len(paths), batch):\n",
        "        chunk = paths[i:i+batch]\n",
        "        Xb, yb = [], []\n",
        "        for p, lab in chunk:\n",
        "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
        "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
        "            Xb.append(arr.reshape(-1)); yb.append(lab)\n",
        "        if Xb:\n",
        "            # ensure we yield float32 arrays (avoids accidental upcast to float64)\n",
        "            yield np.stack(Xb).astype(np.float32, copy=False), np.array(yb)\n",
        "\n",
        "conds = [d for d in os.listdir(ROOT_DEG) if os.path.isdir(os.path.join(ROOT_DEG, d))]\n",
        "\n",
        "# Use a smaller local batch for evaluation to avoid large temporary allocations.\n",
        "# The global BATCH used for training can remain large, but evaluation can use e.g. 128.\n",
        "EVAL_BATCH = min(BATCH, 128)\n",
        "\n",
        "for c in conds:\n",
        "    y_true, y_pred = [], []\n",
        "    any_batch = False\n",
        "    for Xb, yb in stream_degraded_global(c, \"test\", batch=EVAL_BATCH):\n",
        "        any_batch = True\n",
        "        # Xb is already float32 from the generator; keep it that way to reduce memory usage.\n",
        "        Xb_std = scaler.transform(Xb)\n",
        "        Xb_pca = ipca.transform(Xb_std)\n",
        "        yhat_b = clf.predict(Xb_pca)\n",
        "        y_true.extend(yb.tolist())\n",
        "        y_pred.extend(yhat_b.tolist())\n",
        "\n",
        "    if not any_batch:\n",
        "        print(f\"Skip {c}: no files.\")\n",
        "        continue\n",
        "\n",
        "    resg = evaluate_and_pack(np.array(y_true), np.array(y_pred), LABELS)\n",
        "    out = os.path.join(ART, f\"results_logreg_{c}.json\")\n",
        "    with open(out, \"w\") as f:\n",
        "        json.dump(resg, f, indent=2)\n",
        "    print(\"Saved:\", out)\n",
        "\n",
        "print(f\"Total wall time: {(time.time()-t_total)/60:.1f} min\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
