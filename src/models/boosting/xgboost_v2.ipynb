{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda7bd01",
   "metadata": {},
   "source": [
    "# 42177 — XGBoost (clean + degraded), manifest-driven\n",
    "- Uses data/clean splits and manifest.csv label order\n",
    "- Global streaming + StandardScaler + IncrementalPCA (NCOMP=64) for consistency\n",
    "- Saves JSON results to artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c536b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\xlton\\anaconda3\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\xlton\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\xlton\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "# %% 0) Setup\n",
    "# If needed: pip install xgboost\n",
    "!pip install xgboost\n",
    "\n",
    "import os, json, time, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Paths\n",
    "MANIFEST   = r\"D:\\LocalUser\\42177 Project\\data\\manifest.csv\"\n",
    "ROOT_CLEAN = r\"D:\\LocalUser\\42177 Project\\data\\clean\"\n",
    "ROOT_DEG   = r\"D:\\LocalUser\\42177 Project\\data\\degraded\"\n",
    "ART        = r\"D:\\LocalUser\\42177 Project\\artifacts\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "# Hyperparameters (low-RAM defaults)\n",
    "TARGET_HW  = (256, 256)\n",
    "SEED       = 42177\n",
    "BATCH      = 64\n",
    "NCOMP      = 64\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "EXTS = ('.png','.jpg','.jpeg','.tif','.tiff','.bmp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbd9844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['babesia',\n",
       " 'leishmania',\n",
       " 'plasmodium',\n",
       " 'toxoplasma1000x',\n",
       " 'toxoplasma400x',\n",
       " 'trichomonad',\n",
       " 'trypanosome']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% 1) Labels from manifest (fixed order)\n",
    "df = pd.read_csv(MANIFEST)  # expects: id, filepath, label, subset, mag\n",
    "LABELS = sorted(df['label'].unique().tolist())\n",
    "LABELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8910c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 2) Streaming helpers\n",
    "def stream_clean_global(subset, batch=BATCH, target=TARGET_HW):\n",
    "    paths = []\n",
    "    for lab in LABELS:\n",
    "        d = os.path.join(ROOT_CLEAN, subset, lab)\n",
    "        if not os.path.isdir(d): \n",
    "            continue\n",
    "        paths += [(os.path.join(d,f), lab) for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
    "    for i in range(0, len(paths), batch):\n",
    "        chunk = paths[i:i+batch]\n",
    "        Xb, yb = [], []\n",
    "        for p, lab in chunk:\n",
    "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
    "            arr = np.asarray(im, dtype=np.float32)/255.0\n",
    "            Xb.append(arr.reshape(-1)); yb.append(lab)\n",
    "        if Xb:\n",
    "            yield np.stack(Xb), np.array(yb)\n",
    "\n",
    "def stream_degraded_global(cond, subset=\"test\", batch=BATCH, target=TARGET_HW):\n",
    "    base = os.path.join(ROOT_DEG, cond, subset)\n",
    "    if not os.path.isdir(base): \n",
    "        return\n",
    "    paths = []\n",
    "    for lab in LABELS:\n",
    "        d = os.path.join(base, lab)\n",
    "        if not os.path.isdir(d): \n",
    "            continue\n",
    "        paths += [(os.path.join(d,f), lab) for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
    "    for i in range(0, len(paths), batch):\n",
    "        chunk = paths[i:i+batch]\n",
    "        Xb, yb = [], []\n",
    "        for p, lab in chunk:\n",
    "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
    "            arr = np.asarray(im, dtype=np.float32)/255.0\n",
    "            Xb.append(arr.reshape(-1)); yb.append(lab)\n",
    "        if Xb:\n",
    "            yield np.stack(Xb), np.array(yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d95a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3) Metrics packer (aligned with evaluate_model.mlx)\n",
    "def evaluate_and_pack(y_true, y_pred, labels=LABELS):\n",
    "    y_true = np.asarray(y_true).astype(str)\n",
    "    y_pred = np.asarray(y_pred).astype(str)\n",
    "    labels = [str(x) for x in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"Accuracy\": float(acc),\n",
    "        \"Precision\": float(prec),\n",
    "        \"Recall\": float(rec),\n",
    "        \"F1\": float(f1),\n",
    "        \"ConfusionMatrix\": cm.tolist(),\n",
    "        \"Labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e991e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler fitted (global stream).\n",
      "IPCA fitted (n_components=64).\n"
     ]
    }
   ],
   "source": [
    "# %% 4) Fit StandardScaler + IncrementalPCA on TRAIN (global streaming, low RAM)\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "for Xb, _ in stream_clean_global(\"train\"):\n",
    "    scaler.partial_fit(Xb.astype(np.float32, copy=False))\n",
    "    del Xb; gc.collect()\n",
    "print(\"Scaler fitted (global stream).\")\n",
    "\n",
    "ipca = IncrementalPCA(n_components=NCOMP, batch_size=BATCH)\n",
    "for Xb, _ in stream_clean_global(\"train\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "    if Xb_std.shape[0] >= NCOMP:\n",
    "        ipca.partial_fit(Xb_std)\n",
    "    del Xb, Xb_std; gc.collect()\n",
    "print(f\"IPCA fitted (n_components={NCOMP}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc15bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14390, 64), (4810, 64), (4812, 64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% 5) Transform TRAIN/VAL/TEST in streams → compact features\n",
    "def transform_split(split):\n",
    "    Xc, yc = [], []\n",
    "    for Xb, yb in stream_clean_global(split):\n",
    "        Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "        Xb_pca = ipca.transform(Xb_std).astype(np.float32, copy=False)\n",
    "        Xc.append(Xb_pca); yc.append(yb)\n",
    "    X = np.vstack(Xc); y = np.concatenate(yc)\n",
    "    return X, y\n",
    "\n",
    "Xtr, ytr = transform_split(\"train\")\n",
    "Xva, yva = transform_split(\"val\")\n",
    "Xte, yte = transform_split(\"test\")\n",
    "\n",
    "Xtr.shape, Xva.shape, Xte.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6471922",
   "metadata": {},
   "source": [
    "## Train XGBoost (multiclass) — with label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2e44416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost...\n",
      "Training completed in 0.18 minutes\n",
      "Clean test time: 0.0 s\n"
     ]
    }
   ],
   "source": [
    "# %% 6) Train XGBoost (multiclass) — with label encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# after LABELS is defined and Xtr,ytr,Xva,yva,Xte,yte are built (strings)\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(LABELS)  # keep consistent label order\n",
    "\n",
    "ytr_i = le.transform(ytr)\n",
    "yva_i = le.transform(yva)\n",
    "yte_i = le.transform(yte)\n",
    "\n",
    "# train\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=len(LABELS),\n",
    "    n_estimators=400,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "t_train = time.time()\n",
    "xgb.fit(Xtr, ytr_i, eval_set=[(Xva, yva_i)], verbose=False)\n",
    "train_secs = time.time() - t_train\n",
    "print(f\"Training completed in {train_secs/60:.2f} minutes\")\n",
    "\n",
    "# Predict and decode labels back to strings for evaluation\n",
    "t_eval = time.time()\n",
    "yhat_i = xgb.predict(Xte)\n",
    "test_secs = time.time() - t_eval\n",
    "yhat = le.inverse_transform(yhat_i)\n",
    "print(f\"Clean test time: {test_secs:.1f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75df6984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xte shape: (4812, 64)\n",
      "len(yte): 4812 len(yhat): 4812\n",
      "labels: ['babesia', 'leishmania', 'plasmodium', 'toxoplasma1000x', 'toxoplasma400x', 'trichomonad', 'trypanosome']\n",
      "unique yte : ['babesia', 'leishmania', 'plasmodium', 'toxoplasma1000x', 'toxoplasma400x', 'trichomonad', 'trypanosome']\n",
      "unique yhat: ['babesia', 'leishmania', 'plasmodium', 'toxoplasma1000x', 'toxoplasma400x', 'trichomonad', 'trypanosome']\n",
      "missing_from_LABELS_yte : []\n",
      "missing_from_LABELS_yhat: []\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_clean.json\n",
      "Accuracy: 96.07%  F1: 95.45%\n"
     ]
    }
   ],
   "source": [
    "# %% 7) EVALUATE — clean test set\n",
    "# Predict (ints) then decode to strings\n",
    "t_eval = time.time()\n",
    "yhat_i = xgb.predict(Xte)\n",
    "test_secs = time.time() - t_eval\n",
    "yhat = le.inverse_transform(yhat_i)          # strings\n",
    "\n",
    "# --- sanity checks\n",
    "print(\"Xte shape:\", Xte.shape)\n",
    "print(\"len(yte):\", len(yte), \"len(yhat):\", len(yhat))\n",
    "print(\"labels:\", LABELS)\n",
    "\n",
    "yte  = np.asarray(yte).astype(str)\n",
    "yhat = np.asarray(yhat).astype(str)\n",
    "LABELS = [str(x) for x in LABELS]\n",
    "\n",
    "# show mismatches if any\n",
    "uniq_yte  = sorted(set(yte))\n",
    "uniq_yhat = sorted(set(yhat))\n",
    "missing_from_LABELS_yte  = sorted(set(uniq_yte)  - set(LABELS))\n",
    "missing_from_LABELS_yhat = sorted(set(uniq_yhat) - set(LABELS))\n",
    "print(\"unique yte :\", uniq_yte)\n",
    "print(\"unique yhat:\", uniq_yhat)\n",
    "print(\"missing_from_LABELS_yte :\", missing_from_LABELS_yte)\n",
    "print(\"missing_from_LABELS_yhat:\", missing_from_LABELS_yhat)\n",
    "\n",
    "assert len(yte) == len(yhat) and len(yte) > 0\n",
    "assert set(uniq_yte).issubset(set(LABELS))\n",
    "assert set(uniq_yhat).issubset(set(LABELS))\n",
    "\n",
    "# Evaluate and save\n",
    "res_clean = evaluate_and_pack(yte, yhat, LABELS)\n",
    "res_clean[\"TrainElapsedSeconds\"] = round(train_secs, 2)\n",
    "res_clean[\"TestElapsedSeconds\"]  = round(test_secs, 2)\n",
    "\n",
    "out_clean = os.path.join(ART, \"results_xgb_clean.json\")\n",
    "with open(out_clean, \"w\") as f:\n",
    "    json.dump(res_clean, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", out_clean)\n",
    "print(\"Accuracy: %.2f%%  F1: %.2f%%\" % (100*res_clean[\"Accuracy\"], 100*res_clean[\"F1\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adcdd5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 97.23%  Recall: 93.90%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {100*res_clean['Precision']:.2f}%  Recall: {100*res_clean['Recall']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e21e5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating degraded condition: gaussian_blur_s1.0\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_gaussian_blur_s1.0.csv\n",
      "Evaluating degraded condition: gaussian_blur_s2.0\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_gaussian_blur_s2.0.csv\n",
      "Evaluating degraded condition: gaussian_noise_s15\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_gaussian_noise_s15.csv\n",
      "Evaluating degraded condition: gaussian_noise_s5\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_gaussian_noise_s5.csv\n",
      "Evaluating degraded condition: jpeg_q20\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_jpeg_q20.csv\n",
      "Evaluating degraded condition: jpeg_q40\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_jpeg_q40.csv\n",
      "Evaluating degraded condition: jpeg_q60\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_jpeg_q60.csv\n",
      "Evaluating degraded condition: motion_blur_k5\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_motion_blur_k5.csv\n",
      "Evaluating degraded condition: resolution_x2\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_resolution_x2.csv\n",
      "Evaluating degraded condition: resolution_x4\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_resolution_x4.csv\n",
      "Saved index of degraded results: D:\\LocalUser\\42177 Project\\artifacts\\results_xgb_degraded_all.json\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# %% 8) Evaluate on DEGRADED test sets (CSV per condition)\n",
    "def evaluate_degraded_condition(cond, subset=\"test\"):\n",
    "    y_true_all, y_pred_all = [], []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for Xb, yb in stream_degraded_global(cond, subset=subset, batch=BATCH, target=TARGET_HW):\n",
    "        Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "        Xb_pca = ipca.transform(Xb_std).astype(np.float32, copy=False)\n",
    "\n",
    "        yhat_i = xgb.predict(Xb_pca)\n",
    "        yhat = le.inverse_transform(yhat_i)\n",
    "\n",
    "        y_true_all.append(yb.astype(str))\n",
    "        y_pred_all.append(yhat.astype(str))\n",
    "\n",
    "        del Xb, yb, Xb_std, Xb_pca, yhat_i, yhat\n",
    "        gc.collect()\n",
    "\n",
    "    if not y_true_all:\n",
    "        print(f\"[WARN] No samples found for degraded condition '{cond}'\")\n",
    "        return None\n",
    "\n",
    "    y_true = np.concatenate(y_true_all)\n",
    "    y_pred = np.concatenate(y_pred_all)\n",
    "\n",
    "    metrics = evaluate_and_pack(y_true, y_pred, LABELS)\n",
    "    metrics[\"TestElapsedSeconds\"] = round(time.time() - t0, 2)\n",
    "    return metrics\n",
    "\n",
    "degraded_conditions = [\n",
    "    c for c in os.listdir(ROOT_DEG)\n",
    "    if os.path.isdir(os.path.join(ROOT_DEG, c))\n",
    "]\n",
    "\n",
    "all_deg_results = {}\n",
    "for c in degraded_conditions:\n",
    "    print(f\"Evaluating degraded condition: {c}\")\n",
    "    res_deg = evaluate_degraded_condition(c, subset=\"test\")\n",
    "    if res_deg is None:\n",
    "        print(f\"Skipped: {c}\")\n",
    "        continue\n",
    "\n",
    "    all_deg_results[c] = res_deg\n",
    "\n",
    "    # flatten for CSV\n",
    "    out = os.path.join(ART, f\"results_xgb_{c}.csv\")\n",
    "    with open(out, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Metric\", \"Value\"])\n",
    "        writer.writerow([\"Accuracy\", res_deg[\"Accuracy\"]])\n",
    "        writer.writerow([\"Precision\", res_deg[\"Precision\"]])\n",
    "        writer.writerow([\"Recall\", res_deg[\"Recall\"]])\n",
    "        writer.writerow([\"F1\", res_deg[\"F1\"]])\n",
    "        writer.writerow([\"TestElapsedSeconds\", res_deg[\"TestElapsedSeconds\"]])\n",
    "        # confusion matrix as JSON string to keep 1-row format\n",
    "        writer.writerow([\"Labels\", json.dumps(res_deg[\"Labels\"])])\n",
    "        writer.writerow([\"ConfusionMatrix\", json.dumps(res_deg[\"ConfusionMatrix\"])])\n",
    "    print(f\"Saved: {out}\")\n",
    "\n",
    "# optional index (still JSON, per run)\n",
    "idx_path = os.path.join(ART, \"results_xgb_degraded_all.json\")\n",
    "with open(idx_path, \"w\") as f:\n",
    "    json.dump(all_deg_results, f, indent=2)\n",
    "print(f\"Saved index of degraded results: {idx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b75cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% 8) Evaluate on degraded test sets (streaming, low RAM)\n",
    "# conds = [d for d in os.listdir(ROOT_DEG) if os.path.isdir(os.path.join(ROOT_DEG, d))]\n",
    "\n",
    "# for c in conds:\n",
    "#     y_true, y_pred = [], []\n",
    "#     any_batch = False\n",
    "#     for Xb, yb in stream_degraded_global(c, \"test\", batch=BATCH):\n",
    "#         any_batch = True\n",
    "#         Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "#         Xb_pca = ipca.transform(Xb_std)\n",
    "#         yhat_b = xgb.predict(Xb_pca)\n",
    "#         y_true.extend(yb.tolist()); y_pred.extend(yhat_b.tolist())\n",
    "#     if not any_batch:\n",
    "#         print(f\"Skip {c}: no files.\"); continue\n",
    "\n",
    "#     resg = evaluate_and_pack(np.array(y_true), np.array(y_pred), LABELS)\n",
    "#     out = os.path.join(ART, f\"results_xgb_{c}.json\")\n",
    "#     with open(out, \"w\") as f:\n",
    "#         json.dump(resg, f, indent=2)\n",
    "#     print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c1a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% 8) Evaluate on degraded test sets (streaming) - v2\n",
    "# conds = [d for d in os.listdir(ROOT_DEG) if os.path.isdir(os.path.join(ROOT_DEG, d))]\n",
    "# cls_names = np.array([str(c) for c in xgb.classes_])   # use model’s learned order\n",
    "\n",
    "# for c in conds:\n",
    "#     y_true, y_pred = [], []\n",
    "#     any_batch = False\n",
    "\n",
    "#     for Xb, yb in stream_degraded_global(c, \"test\", batch=BATCH):\n",
    "#         any_batch = True\n",
    "#         Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "#         Xb_pca = ipca.transform(Xb_std)\n",
    "\n",
    "#         proba_b = xgb.predict_proba(Xb_pca)\n",
    "#         idx_b   = proba_b.argmax(axis=1)\n",
    "#         y_pred_b = cls_names[idx_b]\n",
    "\n",
    "#         y_true.extend(np.asarray(yb).astype(str).tolist())\n",
    "#         y_pred.extend(y_pred_b.astype(str).tolist())\n",
    "\n",
    "#     if not any_batch:\n",
    "#         print(f\"Skip {c}: no files.\")\n",
    "#         continue\n",
    "\n",
    "#     # quick visibility\n",
    "#     print(f\"[{c}] unique y_true: {sorted(set(y_true))[:3]}\")\n",
    "#     print(f\"[{c}] unique y_pred: {sorted(set(y_pred))[:3]}\")\n",
    "\n",
    "#     ##patch start\n",
    "#     # y_true/y_pred built above...\n",
    "#     y_true = np.asarray(y_true).astype(str)\n",
    "\n",
    "#     y_pred = np.asarray(y_pred)\n",
    "#     # If predictions look like indices ('0','1',...), convert -> ints -> names\n",
    "#     try:\n",
    "#         idx = y_pred.astype(int)\n",
    "#         y_pred_names = np.asarray(cls_names, dtype=str)[idx]\n",
    "#     except ValueError:\n",
    "#         # Not pure indices (already names or probs were mapped earlier)\n",
    "#         y_pred_names = y_pred.astype(str)\n",
    "\n",
    "#     print(f\"[{c}] mapped y_pred sample: {sorted(set(y_pred_names))[:3]}\")\n",
    "#     ##patch end\n",
    "\n",
    "#     resg = evaluate_and_pack(np.array(y_true), np.array(y_pred), cls_names)\n",
    "#     out = os.path.join(ART, f\"results_xgb_{c}.json\")\n",
    "#     with open(out, \"w\") as f:\n",
    "#         json.dump(resg, f, indent=2)\n",
    "#     print(\"Saved:\", out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
