{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99346f4a",
   "metadata": {},
   "source": [
    "# Image Processing and Pattern Recognition - Group Assignment\n",
    "# Feature Extraction + Decision Tree & Random Forest Classifiers\n",
    "\n",
    "By: Shuntian Shi  \n",
    "Purpose: This script reads preprocessed image data from manifest.csv,\n",
    "trains both Decision Tree and Random Forest models,  \n",
    "evaluates performance, and visualizes the results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5ffe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 0) Imports and config\n",
    "import os, json, time, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Paths\n",
    "MANIFEST   = r\"D:\\LocalUser\\42177 Project\\data\\manifest.csv\"\n",
    "ROOT_CLEAN = r\"D:\\LocalUser\\42177 Project\\data\\clean\"\n",
    "ROOT_DEG   = r\"D:\\LocalUser\\42177 Project\\data\\degraded\"\n",
    "ART        = r\"D:\\LocalUser\\42177 Project\\artifacts\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "# Hyperparameters (aligned with LR/SVM/kNN low-RAM defaults)\n",
    "TARGET_HW  = (256, 256)\n",
    "SEED       = 42177\n",
    "BATCH      = 64\n",
    "NCOMP      = 64\n",
    "\n",
    "# Trees\n",
    "DT_PARAMS = dict(criterion=\"gini\", max_depth=None, random_state=SEED)\n",
    "RF_PARAMS = dict(n_estimators=200, max_depth=None, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "EXTS = ('.png','.jpg','.jpeg','.tif','.tiff','.bmp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54233a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['babesia',\n",
       " 'leishmania',\n",
       " 'plasmodium',\n",
       " 'toxoplasma1000x',\n",
       " 'toxoplasma400x',\n",
       " 'trichomonad',\n",
       " 'trypanosome']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% 1) Labels from manifest (fixed order)\n",
    "df = pd.read_csv(MANIFEST)  # expects: id, filepath, label, subset, mag\n",
    "LABELS = sorted(df['label'].unique().tolist())\n",
    "LABELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "425021aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 2) Streaming IO helpers\n",
    "def _list_images(folder):\n",
    "    if not os.path.isdir(folder): return []\n",
    "    return [os.path.join(folder,f) for f in os.listdir(folder) if f.lower().endswith(EXTS)]\n",
    "\n",
    "def stream_clean_global(subset, batch=BATCH, target=TARGET_HW):\n",
    "    paths = []\n",
    "    for lab in LABELS:\n",
    "        d = os.path.join(ROOT_CLEAN, subset, lab)\n",
    "        if not os.path.isdir(d): \n",
    "            continue\n",
    "        paths += [(os.path.join(d, f), lab) for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
    "    for i in range(0, len(paths), batch):\n",
    "        chunk = paths[i:i+batch]\n",
    "        Xb, yb = [], []\n",
    "        for p, lab in chunk:\n",
    "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
    "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "            Xb.append(arr.reshape(-1)); yb.append(lab)\n",
    "        if Xb:\n",
    "            yield np.stack(Xb), np.array(yb)\n",
    "\n",
    "def stream_degraded_global(cond, subset=\"test\", batch=BATCH, target=TARGET_HW):\n",
    "    base = os.path.join(ROOT_DEG, cond, subset)\n",
    "    if not os.path.isdir(base): return\n",
    "    paths = []\n",
    "    for lab in LABELS:\n",
    "        d = os.path.join(base, lab)\n",
    "        if not os.path.isdir(d): \n",
    "            continue\n",
    "        paths += [(os.path.join(d, f), lab) for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
    "    for i in range(0, len(paths), batch):\n",
    "        chunk = paths[i:i+batch]\n",
    "        Xb, yb = [], []\n",
    "        for p, lab in chunk:\n",
    "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
    "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "            Xb.append(arr.reshape(-1)); yb.append(lab)\n",
    "        if Xb:\n",
    "            yield np.stack(Xb), np.array(yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e70e005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3) Metrics packer (aligned with evaluate_model.mlx)\n",
    "def evaluate_and_pack(y_true, y_pred, labels=LABELS):\n",
    "    y_true = np.asarray(y_true).astype(str)\n",
    "    y_pred = np.asarray(y_pred).astype(str)\n",
    "    labels = [str(x) for x in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"Accuracy\": float(acc),\n",
    "        \"Precision\": float(prec),\n",
    "        \"Recall\": float(rec),\n",
    "        \"F1\": float(f1),\n",
    "        \"ConfusionMatrix\": cm.tolist(),\n",
    "        \"Labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365e59c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler fitted (global stream).\n",
      "IPCA fitted (n_components=64).\n"
     ]
    }
   ],
   "source": [
    "# %% 4) Fit StandardScaler + IncrementalPCA (global stream, low RAM)\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "for Xb, _ in stream_clean_global(\"train\"):\n",
    "    scaler.partial_fit(Xb.astype(np.float32, copy=False))\n",
    "    del Xb; gc.collect()\n",
    "print(\"Scaler fitted (global stream).\")\n",
    "\n",
    "ipca = IncrementalPCA(n_components=NCOMP, batch_size=BATCH)\n",
    "for Xb, _ in stream_clean_global(\"train\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "    if Xb_std.shape[0] >= NCOMP:\n",
    "        ipca.partial_fit(Xb_std)\n",
    "    del Xb, Xb_std; gc.collect()\n",
    "print(f\"IPCA fitted (n_components={NCOMP}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bd20400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 5) Transform TRAIN/TEST in streams â†’ compact features\n",
    "Xtr_chunks, ytr_chunks = [], []\n",
    "for Xb, yb in stream_clean_global(\"train\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "    Xb_pca = ipca.transform(Xb_std)\n",
    "    Xtr_chunks.append(Xb_pca.astype(np.float32, copy=False))\n",
    "    ytr_chunks.append(yb)\n",
    "Xtr = np.vstack(Xtr_chunks); ytr = np.concatenate(ytr_chunks)\n",
    "\n",
    "Xte_chunks, yte_chunks = [], []\n",
    "for Xb, yb in stream_clean_global(\"test\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "    Xb_pca = ipca.transform(Xb_std)\n",
    "    Xte_chunks.append(Xb_pca.astype(np.float32, copy=False))\n",
    "    yte_chunks.append(yb)\n",
    "Xte = np.vstack(Xte_chunks); yte = np.concatenate(yte_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9263a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree...\n",
      "DT training time: 0.03 min\n",
      "Training Random Forest...\n",
      "RF training time: 0.06 min\n"
     ]
    }
   ],
   "source": [
    "# %% 6) Train Decision Tree and Random Forest\n",
    "dt = DecisionTreeClassifier(**DT_PARAMS)\n",
    "rf = RandomForestClassifier(**RF_PARAMS)\n",
    "\n",
    "print(\"Training Decision Tree...\")\n",
    "t0 = time.time(); dt.fit(Xtr, ytr); dt_secs = time.time() - t0\n",
    "print(f\"DT training time: {dt_secs/60:.2f} min\")\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "t0 = time.time(); rf.fit(Xtr, ytr); rf_secs = time.time() - t0\n",
    "print(f\"RF training time: {rf_secs/60:.2f} min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b08609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_clean.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_clean.json\n"
     ]
    }
   ],
   "source": [
    "# %% 7) Evaluate on clean test and save JSON\n",
    "# Decision Tree\n",
    "t1 = time.time(); yhat_dt = dt.predict(Xte); dt_test = time.time() - t1\n",
    "res_dt = evaluate_and_pack(yte, yhat_dt, LABELS)\n",
    "res_dt[\"TrainElapsedSeconds\"] = round(dt_secs, 2)\n",
    "res_dt[\"TestElapsedSeconds\"]  = round(dt_test, 2)\n",
    "out_dt = os.path.join(ART, \"results_tree_clean.json\")\n",
    "with open(out_dt, \"w\") as f: json.dump(res_dt, f, indent=2)\n",
    "print(\"Saved:\", out_dt)\n",
    "\n",
    "# Random Forest\n",
    "t1 = time.time(); yhat_rf = rf.predict(Xte); rf_test = time.time() - t1\n",
    "res_rf = evaluate_and_pack(yte, yhat_rf, LABELS)\n",
    "res_rf[\"TrainElapsedSeconds\"] = round(rf_secs, 2)\n",
    "res_rf[\"TestElapsedSeconds\"]  = round(rf_test, 2)\n",
    "out_rf = os.path.join(ART, \"results_rf_clean.json\")\n",
    "with open(out_rf, \"w\") as f: json.dump(res_rf, f, indent=2)\n",
    "print(\"Saved:\", out_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f015dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_gaussian_blur_s1.0.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_gaussian_blur_s1.0.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_gaussian_blur_s2.0.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_gaussian_blur_s2.0.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_gaussian_noise_s15.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_gaussian_noise_s15.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_gaussian_noise_s5.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_gaussian_noise_s5.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_jpeg_q20.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_jpeg_q20.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_jpeg_q40.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_jpeg_q40.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_jpeg_q60.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_jpeg_q60.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_motion_blur_k5.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_motion_blur_k5.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_resolution_x2.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_resolution_x2.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_tree_resolution_x4.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_rf_resolution_x4.json\n"
     ]
    }
   ],
   "source": [
    "# %% 8) Evaluate on degraded test sets (streaming)\n",
    "conds = [d for d in os.listdir(ROOT_DEG) if os.path.isdir(os.path.join(ROOT_DEG, d))]\n",
    "\n",
    "for c in conds:\n",
    "    # Decision Tree\n",
    "    y_true, y_pred = [], []\n",
    "    any_batch = False\n",
    "    for Xb, yb in stream_degraded_global(c, \"test\", batch=BATCH):\n",
    "        any_batch = True\n",
    "        Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "        Xb_pca = ipca.transform(Xb_std)\n",
    "        y_pred.extend(dt.predict(Xb_pca).tolist())\n",
    "        y_true.extend(yb.tolist())\n",
    "    if any_batch:\n",
    "        res = evaluate_and_pack(np.array(y_true), np.array(y_pred), LABELS)\n",
    "        out = os.path.join(ART, f\"results_tree_{c}.json\")\n",
    "        with open(out, \"w\") as f: json.dump(res, f, indent=2)\n",
    "        print(\"Saved:\", out)\n",
    "\n",
    "    # Random Forest\n",
    "    y_true, y_pred = [], []\n",
    "    any_batch = False\n",
    "    for Xb, yb in stream_degraded_global(c, \"test\", batch=BATCH):\n",
    "        any_batch = True\n",
    "        Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "        Xb_pca = ipca.transform(Xb_std)\n",
    "        y_pred.extend(rf.predict(Xb_pca).tolist())\n",
    "        y_true.extend(yb.tolist())\n",
    "    if any_batch:\n",
    "        res = evaluate_and_pack(np.array(y_true), np.array(y_pred), LABELS)\n",
    "        out = os.path.join(ART, f\"results_rf_{c}.json\")\n",
    "        with open(out, \"w\") as f: json.dump(res, f, indent=2)\n",
    "        print(\"Saved:\", out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
