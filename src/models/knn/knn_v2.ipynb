{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49540624",
   "metadata": {},
   "source": [
    "# 42177 Image Processing and Pattern Recognition  \n",
    "## k-Nearest Neighbors (kNN) Model – PCA + Scaler Pipeline  \n",
    "\n",
    "### Overview  \n",
    "This notebook implements a **k-Nearest Neighbors (kNN)** classifier for the parasite image classification project.  \n",
    "It follows the **same preprocessing and evaluation framework** as the Logistic Regression and SVM models to ensure fair, consistent comparison across all team models.\n",
    "\n",
    "### Key Features  \n",
    "- Uses the shared **manifest.csv** and **data/clean** folder structure (train/val/test splits fixed).  \n",
    "- Applies **global streaming** for memory-safe processing.  \n",
    "- Employs **StandardScaler** and **IncrementalPCA (NCOMP=64)** for feature standardisation and dimensionality reduction.  \n",
    "- Trains and evaluates a **kNN classifier (k=5, distance-weighted, Euclidean)** on compact PCA features.  \n",
    "- Produces evaluation metrics identical to `evaluate_model.mlx` in MATLAB (Accuracy, Precision, Recall, F1).  \n",
    "- Saves JSON results to the `artifacts/` folder for clean and degraded test sets.\n",
    "\n",
    "### Workflow  \n",
    "1. Load manifest and define label order.  \n",
    "2. Stream training data → fit StandardScaler and IncrementalPCA incrementally.  \n",
    "3. Transform train/test sets in batches to avoid memory overflow.  \n",
    "4. Train kNN on PCA-reduced features.  \n",
    "5. Evaluate on clean and degraded test sets.  \n",
    "6. Save results (`results_knn_clean.json`, `results_knn_<degradation>.json`).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd164c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 0) Imports and config\n",
    "import os, json, time, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Paths\n",
    "MANIFEST   = r\"D:\\LocalUser\\42177 Project\\data\\manifest.csv\"\n",
    "ROOT_CLEAN = r\"D:\\LocalUser\\42177 Project\\data\\clean\"\n",
    "ROOT_DEG   = r\"D:\\LocalUser\\42177 Project\\data\\degraded\"\n",
    "ART        = r\"D:\\LocalUser\\42177 Project\\artifacts\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "# Hyperparameters (kept small for 16 GB RAM)\n",
    "TARGET_HW  = (256, 256)\n",
    "SEED       = 42177\n",
    "BATCH      = 64\n",
    "NCOMP      = 64\n",
    "K          = 5              # kNN neighbors\n",
    "WEIGHTS    = \"distance\"     # \"uniform\" or \"distance\"\n",
    "METRIC     = \"euclidean\"    # distance metric\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "EXTS = ('.png','.jpg','.jpeg','.tif','.tiff','.bmp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a456d8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['babesia',\n",
       " 'leishmania',\n",
       " 'plasmodium',\n",
       " 'toxoplasma1000x',\n",
       " 'toxoplasma400x',\n",
       " 'trichomonad',\n",
       " 'trypanosome']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% 1) Labels from manifest (fixed order)\n",
    "df = pd.read_csv(MANIFEST)   # expects: id, filepath, label, subset, mag\n",
    "LABELS = sorted(df['label'].unique().tolist())\n",
    "LABELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69531275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 2) Streaming IO helpers\n",
    "def _list_images(folder):\n",
    "    if not os.path.isdir(folder): return []\n",
    "    return [os.path.join(folder,f) for f in os.listdir(folder) if f.lower().endswith(EXTS)]\n",
    "\n",
    "def stream_clean_global(subset, batch=BATCH, target=TARGET_HW):\n",
    "    \"\"\"Yield (Xb, yb) batches from data/clean/<subset>/<label>, RGB→[0,1], flattened.\"\"\"\n",
    "    paths = []\n",
    "    for lab in LABELS:\n",
    "        d = os.path.join(ROOT_CLEAN, subset, lab)\n",
    "        if not os.path.isdir(d): \n",
    "            continue\n",
    "        paths += [(os.path.join(d, f), lab) for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
    "    for i in range(0, len(paths), batch):\n",
    "        chunk = paths[i:i+batch]\n",
    "        Xb, yb = [], []\n",
    "        for p, lab in chunk:\n",
    "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
    "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "            Xb.append(arr.reshape(-1)); yb.append(lab)\n",
    "        if Xb:\n",
    "            yield np.stack(Xb), np.array(yb)\n",
    "\n",
    "def stream_degraded_global(cond, subset=\"test\", batch=BATCH, target=TARGET_HW):\n",
    "    base = os.path.join(ROOT_DEG, cond, subset)\n",
    "    if not os.path.isdir(base): \n",
    "        return\n",
    "    paths = []\n",
    "    for lab in LABELS:\n",
    "        d = os.path.join(base, lab)\n",
    "        if not os.path.isdir(d): \n",
    "            continue\n",
    "        paths += [(os.path.join(d, f), lab) for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
    "    for i in range(0, len(paths), batch):\n",
    "        chunk = paths[i:i+batch]\n",
    "        Xb, yb = [], []\n",
    "        for p, lab in chunk:\n",
    "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
    "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "            Xb.append(arr.reshape(-1)); yb.append(lab)\n",
    "        if Xb:\n",
    "            yield np.stack(Xb), np.array(yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ad7232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3) Metrics packer (aligned with evaluate_model.mlx)\n",
    "def evaluate_and_pack(y_true, y_pred, labels=LABELS):\n",
    "    y_true = np.asarray(y_true).astype(str)\n",
    "    y_pred = np.asarray(y_pred).astype(str)\n",
    "    labels = [str(x) for x in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"Accuracy\": float(acc),\n",
    "        \"Precision\": float(prec),\n",
    "        \"Recall\": float(rec),\n",
    "        \"F1\": float(f1),\n",
    "        \"ConfusionMatrix\": cm.tolist(),\n",
    "        \"Labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13b8588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler fitted (global stream).\n",
      "IPCA fitted (n_components=64).\n"
     ]
    }
   ],
   "source": [
    "# %% 4) Fit StandardScaler + IncrementalPCA (global stream, low RAM)\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "for Xb, _ in stream_clean_global(\"train\"):\n",
    "    scaler.partial_fit(Xb.astype(np.float32, copy=False))\n",
    "    del Xb; gc.collect()\n",
    "print(\"Scaler fitted (global stream).\")\n",
    "\n",
    "ipca = IncrementalPCA(n_components=NCOMP, batch_size=BATCH)\n",
    "for Xb, _ in stream_clean_global(\"train\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))  # may be float64 inside\n",
    "    if Xb_std.shape[0] >= NCOMP:\n",
    "        ipca.partial_fit(Xb_std)\n",
    "    del Xb, Xb_std; gc.collect()\n",
    "print(f\"IPCA fitted (n_components={NCOMP}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a41d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 5) Transform TRAIN/TEST in streams → compact features\n",
    "Xtr_chunks, ytr_chunks = [], []\n",
    "for Xb, yb in stream_clean_global(\"train\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "    Xb_pca = ipca.transform(Xb_std)\n",
    "    Xtr_chunks.append(Xb_pca.astype(np.float32, copy=False))\n",
    "    ytr_chunks.append(yb)\n",
    "Xtr = np.vstack(Xtr_chunks); ytr = np.concatenate(ytr_chunks)\n",
    "\n",
    "Xte_chunks, yte_chunks = [], []\n",
    "for Xb, yb in stream_clean_global(\"test\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "    Xb_pca = ipca.transform(Xb_std)\n",
    "    Xte_chunks.append(Xb_pca.astype(np.float32, copy=False))\n",
    "    yte_chunks.append(yb)\n",
    "Xte = np.vstack(Xte_chunks); yte = np.concatenate(yte_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28bdd838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training kNN...\n",
      "Training completed in 0.00 minutes\n"
     ]
    }
   ],
   "source": [
    "# %% 6) Train kNN on PCA features\n",
    "knn = KNeighborsClassifier(n_neighbors=K, weights=WEIGHTS, metric=METRIC, n_jobs=-1)\n",
    "\n",
    "print(\"Training kNN...\")\n",
    "t_train = time.time()\n",
    "knn.fit(Xtr, ytr)\n",
    "train_secs = time.time() - t_train\n",
    "print(f\"Training completed in {train_secs/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2798f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_clean.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.9416043225270158,\n",
       " 'Precision': 0.953594793203548,\n",
       " 'Recall': 0.9207970920035234,\n",
       " 'F1': 0.936050782482773,\n",
       " 'ConfusionMatrix': [[242, 6, 0, 1, 0, 5, 1],\n",
       "  [0, 427, 0, 3, 1, 106, 4],\n",
       "  [3, 3, 153, 0, 0, 11, 0],\n",
       "  [2, 2, 1, 574, 1, 6, 2],\n",
       "  [0, 0, 0, 1, 752, 0, 0],\n",
       "  [0, 55, 0, 0, 0, 1973, 0],\n",
       "  [0, 18, 0, 19, 0, 30, 410]],\n",
       " 'Labels': ['babesia',\n",
       "  'leishmania',\n",
       "  'plasmodium',\n",
       "  'toxoplasma1000x',\n",
       "  'toxoplasma400x',\n",
       "  'trichomonad',\n",
       "  'trypanosome'],\n",
       " 'TrainElapsedSeconds': 0.0,\n",
       " 'TestElapsedSeconds': 0.29}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% 7) Evaluate on clean test and save JSON\n",
    "t_eval = time.time()\n",
    "yhat = knn.predict(Xte)\n",
    "test_secs = time.time() - t_eval\n",
    "\n",
    "res_clean = evaluate_and_pack(yte, yhat, LABELS)\n",
    "res_clean[\"TrainElapsedSeconds\"] = round(train_secs, 2)\n",
    "res_clean[\"TestElapsedSeconds\"]  = round(test_secs, 2)\n",
    "\n",
    "out_clean = os.path.join(ART, \"results_knn_clean.json\")\n",
    "with open(out_clean, \"w\") as f:\n",
    "    json.dump(res_clean, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", out_clean)\n",
    "res_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "150cd351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_gaussian_blur_s1.0.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_gaussian_blur_s2.0.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_gaussian_noise_s15.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_gaussian_noise_s5.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_jpeg_q20.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_jpeg_q40.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_jpeg_q60.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_motion_blur_k5.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_resolution_x2.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_knn_resolution_x4.json\n"
     ]
    }
   ],
   "source": [
    "# %% 8) Evaluate on degraded test sets (streaming)\n",
    "conds = [d for d in os.listdir(ROOT_DEG) if os.path.isdir(os.path.join(ROOT_DEG, d))]\n",
    "\n",
    "for c in conds:\n",
    "    y_true, y_pred = [], []\n",
    "    any_batch = False\n",
    "    for Xb, yb in stream_degraded_global(c, \"test\", batch=BATCH):\n",
    "        any_batch = True\n",
    "        Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "        Xb_pca = ipca.transform(Xb_std)\n",
    "        yhat_b = knn.predict(Xb_pca)\n",
    "        y_true.extend(yb.tolist()); y_pred.extend(yhat_b.tolist())\n",
    "    if not any_batch:\n",
    "        print(f\"Skip {c}: no files.\"); continue\n",
    "\n",
    "    resg = evaluate_and_pack(np.array(y_true), np.array(y_pred), LABELS)\n",
    "    out = os.path.join(ART, f\"results_knn_{c}.json\")\n",
    "    with open(out, \"w\") as f:\n",
    "        json.dump(resg, f, indent=2)\n",
    "    print(\"Saved:\", out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
