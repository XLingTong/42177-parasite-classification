{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27f4a114",
   "metadata": {},
   "source": [
    "# 42177 — PCA → SVM (clean + degraded), manifest-driven\n",
    "- Global streaming for StandardScaler + IncrementalPCA\n",
    "- BATCH=64, NCOMP=64\n",
    "- Batch transform to avoid large allocations\n",
    "- Results save JSON to artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 0) Imports and config\n",
    "import os, json, time, random, numpy as np, pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Paths\n",
    "MANIFEST   = r\"D:\\LocalUser\\42177 Project\\data\\manifest.csv\"\n",
    "ROOT_CLEAN = r\"D:\\LocalUser\\42177 Project\\data\\clean\"\n",
    "ROOT_DEG   = r\"D:\\LocalUser\\42177 Project\\data\\degraded\"\n",
    "ART        = r\"D:\\LocalUser\\42177 Project\\artifacts\"\n",
    "os.makedirs(ART, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "TARGET_HW  = (256, 256)\n",
    "SEED       = 42177\n",
    "BATCH      = 64\n",
    "NCOMP      = 64\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "EXTS = ('.png','.jpg','.jpeg','.tif','.tiff','.bmp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "481e2367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['babesia',\n",
       " 'leishmania',\n",
       " 'plasmodium',\n",
       " 'toxoplasma1000x',\n",
       " 'toxoplasma400x',\n",
       " 'trichomonad',\n",
       " 'trypanosome']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% 1) Labels from manifest (fixed order)\n",
    "df = pd.read_csv(MANIFEST)  # expects: id, filepath, label, subset, mag\n",
    "LABELS = sorted(df['label'].unique().tolist())\n",
    "LABELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e74997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 2) I/O streaming (global stream)\n",
    "def _list_images(folder):\n",
    "    if not os.path.isdir(folder): return []\n",
    "    return [os.path.join(folder,f) for f in os.listdir(folder) if f.lower().endswith(EXTS)]\n",
    "\n",
    "def stream_clean_global(subset, batch=BATCH, target=TARGET_HW):\n",
    "    paths = []\n",
    "    for lab in LABELS:\n",
    "        d = os.path.join(ROOT_CLEAN, subset, lab)\n",
    "        if not os.path.isdir(d): \n",
    "            continue\n",
    "        paths += [(os.path.join(d, f), lab) for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
    "    for i in range(0, len(paths), batch):\n",
    "        chunk = paths[i:i+batch]\n",
    "        Xb, yb = [], []\n",
    "        for p, lab in chunk:\n",
    "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
    "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "            Xb.append(arr.reshape(-1)); yb.append(lab)\n",
    "        yield np.stack(Xb), np.array(yb)\n",
    "\n",
    "def stream_degraded_global(cond, subset=\"test\", batch=BATCH, target=TARGET_HW):\n",
    "    base = os.path.join(ROOT_DEG, cond, subset)\n",
    "    if not os.path.isdir(base):\n",
    "        return\n",
    "    paths = []\n",
    "    for lab in LABELS:\n",
    "        d = os.path.join(base, lab)\n",
    "        if not os.path.isdir(d): \n",
    "            continue\n",
    "        paths += [(os.path.join(d, f), lab) for f in os.listdir(d) if f.lower().endswith(EXTS)]\n",
    "    for i in range(0, len(paths), batch):\n",
    "        chunk = paths[i:i+batch]\n",
    "        Xb, yb = [], []\n",
    "        for p, lab in chunk:\n",
    "            im = Image.open(p).convert(\"RGB\").resize(target)\n",
    "            arr = np.asarray(im, dtype=np.float32) / 255.0\n",
    "            Xb.append(arr.reshape(-1)); yb.append(lab)\n",
    "        if Xb:\n",
    "            yield np.stack(Xb), np.array(yb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f90efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 3) Metrics packer (aligned with evaluate_model.mlx)\n",
    "def evaluate_and_pack(y_true, y_pred, labels=LABELS):\n",
    "    y_true = np.asarray(y_true).astype(str)\n",
    "    y_pred = np.asarray(y_pred).astype(str)\n",
    "    labels = [str(x) for x in labels]\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    acc = (y_true == y_pred).mean().item()\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=labels, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"Accuracy\": float(acc),\n",
    "        \"Precision\": float(prec),\n",
    "        \"Recall\": float(rec),\n",
    "        \"F1\": float(f1),\n",
    "        \"ConfusionMatrix\": cm.tolist(),\n",
    "        \"Labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73485469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler fitted (global).\n",
      "IPCA fitted (n_components=64).\n"
     ]
    }
   ],
   "source": [
    "# %% 4) Fit StandardScaler + IncrementalPCA (global stream, low RAM)\n",
    "from sklearn.utils import shuffle\n",
    "import gc\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "for Xb, _ in stream_clean_global(\"train\"):\n",
    "    scaler.partial_fit(Xb.astype(np.float32, copy=False))\n",
    "    del Xb; gc.collect()\n",
    "print(\"Scaler fitted (global).\")\n",
    "\n",
    "ipca = IncrementalPCA(n_components=NCOMP, batch_size=BATCH)\n",
    "for Xb, _ in stream_clean_global(\"train\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "    if Xb_std.shape[0] >= NCOMP:\n",
    "        ipca.partial_fit(Xb_std)\n",
    "    del Xb, Xb_std; gc.collect()\n",
    "print(f\"IPCA fitted (n_components={NCOMP}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d0ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 5) Transform TRAIN/TEST in streams → compact features\n",
    "Xtr_chunks, ytr_chunks = [], []\n",
    "for Xb, yb in stream_clean_global(\"train\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "    Xb_pca = ipca.transform(Xb_std)\n",
    "    Xtr_chunks.append(Xb_pca.astype(np.float32, copy=False))\n",
    "    ytr_chunks.append(yb)\n",
    "Xtr = np.vstack(Xtr_chunks); ytr = np.concatenate(ytr_chunks)\n",
    "\n",
    "Xte_chunks, yte_chunks = [], []\n",
    "for Xb, yb in stream_clean_global(\"test\"):\n",
    "    Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "    Xb_pca = ipca.transform(Xb_std)\n",
    "    Xte_chunks.append(Xb_pca.astype(np.float32, copy=False))\n",
    "    yte_chunks.append(yb)\n",
    "Xte = np.vstack(Xte_chunks); yte = np.concatenate(yte_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431dcc0",
   "metadata": {},
   "source": [
    "## Train Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86962f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eef420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear SVM...\n",
      "Training completed in 0.04 minutes\n"
     ]
    }
   ],
   "source": [
    "# %% 6) Train Linear SVM (on PCA features)\n",
    "svm = LinearSVC(C=1.0, loss='squared_hinge', dual=False, max_iter=2000)\n",
    "import time\n",
    "print(\"Training Linear SVM...\")\n",
    "t_train = time.time()\n",
    "svm.fit(Xtr, ytr)\n",
    "train_secs = time.time() - t_train\n",
    "print(f\"Training completed in {train_secs/60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe27ae5",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81535c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_clean.json\n"
     ]
    }
   ],
   "source": [
    "# %% 7) Evaluate on clean test set and save\n",
    "t_eval = time.time()\n",
    "yhat = svm.predict(Xte)\n",
    "test_secs = time.time() - t_eval\n",
    "\n",
    "res_clean = evaluate_and_pack(yte, yhat, LABELS)\n",
    "res_clean[\"TrainElapsedSeconds\"] = round(train_secs, 2)\n",
    "res_clean[\"TestElapsedSeconds\"]  = round(test_secs, 2)\n",
    "\n",
    "out_clean = os.path.join(ART, \"results_svm_clean.json\")\n",
    "with open(out_clean, \"w\") as f:\n",
    "    json.dump(res_clean, f, indent=2)\n",
    "print(\"Saved:\", out_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecc72964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_gaussian_blur_s1.0.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_gaussian_blur_s2.0.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_gaussian_noise_s15.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_gaussian_noise_s5.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_jpeg_q20.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_jpeg_q40.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_jpeg_q60.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_motion_blur_k5.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_resolution_x2.json\n",
      "Saved: D:\\LocalUser\\42177 Project\\artifacts\\results_svm_resolution_x4.json\n"
     ]
    }
   ],
   "source": [
    "# %% 8) Evaluate on degraded test sets (streaming)\n",
    "conds = [d for d in os.listdir(ROOT_DEG) if os.path.isdir(os.path.join(ROOT_DEG, d))]\n",
    "for c in conds:\n",
    "    y_true, y_pred = [], []\n",
    "    any_batch = False\n",
    "    for Xb, yb in stream_degraded_global(c, \"test\", batch=BATCH):\n",
    "        any_batch = True\n",
    "        Xb_std = scaler.transform(Xb.astype(np.float32, copy=False))\n",
    "        Xb_pca = ipca.transform(Xb_std)\n",
    "        yhat_b = svm.predict(Xb_pca)\n",
    "        y_true.extend(yb.tolist()); y_pred.extend(yhat_b.tolist())\n",
    "    if not any_batch:\n",
    "        print(f\"Skip {c}: no files.\"); continue\n",
    "    resg = evaluate_and_pack(np.array(y_true), np.array(y_pred), LABELS)\n",
    "    out = os.path.join(ART, f\"results_svm_{c}.json\")\n",
    "    with open(out, \"w\") as f: json.dump(resg, f, indent=2)\n",
    "    print(\"Saved:\", out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
